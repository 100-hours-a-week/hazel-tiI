## 날짜: 2025-02-07

## 운영체제 : Lock


<br>


### 락 (Lock)
- 하나의 스레드/프로세스가 자원을 점유할 때 다른 스레드/프로세스가 해당 자원에 접근하지 못하도록 제어하는 것 
#### 락의 등장 배경 
- 멀티 스레드 환경에서는 여러 스레드가 동시에 실행 되면서 공유 데이터에 대한 동기화가 필요함 
- 동기화 문제를 해결하기 위해 mutual exclusion 방법을 도입하였고 이를 보장하는 방법 중 하나가 Lock 
- 스레드가 동시에 접근하면 안되는 코드 영역을 Critical Section이라고 하고 임계 영역에서는 Lock을 사용해서 mutual exclusion을 보장함
#### 락을 사용한 문제 예방  
- 데이터 무결성 손실 방지 : 여러 스레드가 동시에 데이터를 수정하면서 발생하는 무결성 문제 해결  
- 원자성 보장 : 연산 도중 중간 상태가 발생하지 않도록 보장 
- 데드락 방지 : 락 사용 및 해제를 통해 교착 상태를 예방 
- 레이스 컨디션 방지 : 다수의 스레드가 동일한 자원에 접근하면서 발생하는 동기화 문제 해결 
#### 락의 동작 방식 
- 공유 자원 비활성 상태
- 스레드1이 락 요청 -> 락 점유 및 작업 수행  
- 스레드2가 락 요청 -> 락이 점유 상태 이므로 대기
- 스레드1 락 해제 -> 작업 완료 후 락 해제 
- 스레드2가 락 요청 -> 공유 자원에 접근해 락을 점유하고 작업 수행 


<br>


### 임계 영역 (Critical Section)
- 여러 스레드가 동시에 접근하면 안 되는 공유 데이터가 존재하는 코드 영역
- 스레드 간 동기화 문제를 방지하기 위해 Lock을 적용하는 영역임 
#### Critical Section Problem 
- Mutual Exclusion : 한 번에 하나의 스레드만 임계 영역에 들어가는 것을 보장 
- Progress : 한 스레드가 임계 영역에 진입하기 위해 대기하고 있을 때 임계 영역을 점유 중인 스레드가 없으면 대기중인 스레드가 임계 영역에 진입하는 것을 보장
- Bounded Waiting : 한 스레드가 임계 영역에 진입하기 위해 대기하는 시간은 무한정이 아닌 제한된 시간이어야 함 


<br>

### 동기화
- 여러 스레드가 공유 자원에 동시에 접근하는 것을 조절하는 기법  
- 스핀락, 뮤텍스, 세마포어 등 다양한 동기화 방법이 존재함  

<br>

### 스핀락 (Spin Lock)
- 락을 점유한 스레드가 있을 경우, 다른 스레드가 계속해서 락이 해제되었는지 확인하는 방식 (busy-waiting)
- 락 점유 시간이 짧고 빠른 응답이 필요한 경우에 사용하는 것이 적합
#### 스핀락의 장점
- 락 점유 해제가 빠름 : 락이 점유상태일 때 대기 상태로 전환하지 않고 락 상태를 계속해서 확인하기 때문에 락이 해제될 경우 바로 점유가 가능함 
- context switching 없음 : 락이 빠르게 해제될 경우 오버헤드가 적음 
- 간단한 구현 
#### 스핀락의 단점 
- 락의 상태를 계속 확인하기 때문에 그만큼 CPU 자원을 많이 사용함
- 락 점유 시간이 길 경우 비효율적임 
- 단일 CPU 환경에서는 락을 점유한 스레드와 대기 중인 스레드가 동시에 실행될 수 없음 

<br>

### 낙관적 락 (Optimistic Lock)
- 데이터 충돌이 적을 것이라고 가정하고 충돌 발생 시 롤백으로 해결하는 방식  
- 충돌 발생 시 충돌이 감지된 스레드 중 현재 변경 작업이 우선순위가 더 낮은 작업 또는 충돌 감지 시점 이전 작업을 롤백
#### 낙관적 락의 장점 
- 충돌 발생이 적은 환경에서 성능이 뛰어남 
- 락 관리에 따른 오버헤드가 거의 없음 
- 여러 스레드가 동시에 공유 자원에 접근할 수 있으므로 데이터 경합이 적은 경우 병렬 처리 성능이 뛰어남 
#### 낙관점 락의 단점 
- 데이터 충돌 시 롤백을 통한 비용 발생 
- 데이터 경합이 자주 발생하면 성능이 저하됨 
- 충돌 감지 및 롤백 매커니즘으로 인해 설계 구현 복잡도가 올라감 

<br>

### 뮤텍스 (Mutex)
- 특정 자원을 여러 스레드/프로세스가 동시에 접근하지 못하도록 하여 레이스 컨디션을 방지하는 방법 
- 한 번에 하나의 스레드만 특정 자원에 접근 가능 
- 락의 소유권과 해제 권한을 특정 스레드에 부여함
#### 뮤텍스의 동작 방식
- 공유 자원 비활성 상태 
- 스레드1이 락 요청 -> 공유 자원을 점유하고 작업 시작
- 스레드2가 락 요청 -> 락이 점유 상태이므로 OS의 뮤텍스 대기 큐(FIFO)에서 대기 상태로 전환됨 
- 스레드1이 락 해제 : 뮤텍스 대기 큐의 첫 번째 스레드로 notify 신호가 전달 됨
- 스레드2 락 : 대기 중이던 스레드2가 공유 자원을 점유하고 작업 시작 
#### 뮤텍스의 장점
- 한 번에 하나의 스레드만 사용하도록 제한하기 때문에 데이터 무결성을 보장
- 동기화 문제가 발생할 가능성이 낮음
#### 뮤텍스의 단점 
- 컨텍스트 스위칭으로 인한 오버헤드가 발생할 수 있음 
- 락을 해제하지 않거나 여러 락을 잘못된 순서로 요청 시 데드락 발생 가능 
- 한 번에 하나의 스레드만 자원을 사용할 수 있어 병렬 처리가 어려움 
#### 뮤텍스와 조건변수 (Mutex & Condition Variable)
- 조건 변수는 특정 조건을 만족할 때까지 스레드가 대기하도록 하는 동기화 기법 
- 뮤텍스와 조건 변수를 함께 사용하면 스레드가 특정 이벤트가 발생할 때까지 효율적으로 대기 가능


<br>


### 리드-라이트 락 (Read-Write Lock)
- 다중 스레드 환경에서 읽기와 쓰기 작업을 구분해서 락을 관리하는 방식 
- 여러 Reader는 동시에 자원에 접근할 수 있지만 Writer는 자원을 독점적으로 사용해야 함 
#### Read Lock
#### 1. 읽기 요청 
- 스레드1이 읽기 요청 시 : Read Lock 점유 Reader Count는 1로 증가 
- 스레드2가 읽기 요청 시 : 두 스레드가 공유 자원을 동시에 점유 Reader Count는 2로 증가  
#### 2. 쓰기 요청 
- 스레드1이 쓰기 요청 시 : 스레드1이 Read Lock 점유 
- 스레드2가 쓰기 요청 시 : 스레드1이 Read Lock을 점유 중이므로 Witer 대기큐에 대기함 
- Read Lock 해제 : Writer 대기큐에 notify가 전달되고 스레드2가 Read Lock 점유
#### Write Lock 
#### 1. 읽기 요청
- 스레드1이 읽기 요청 시 : 스레드1이 Write Lock 점유  
- 스레드2가 읽기 요청 시 : 스레드1이 Write Lock을 점유 중이므로 Reader 대기 큐에 대기함 
- Writer Lcok 해제 : Reader 대기큐에 notify가 전달되고 스레드2가 Writer Lock 점유  
#### 2. 쓰기 요청
- 스레드1이 쓰기 요청 시 : 스레드1이 Writer Lock 점유 
- 스레드2가 쓰기 요청 시 : 스레드1이 Write Lock을 점유 중이므로 Writer 대기 큐에 대기함
- Write Lock 해제 : Writer 대기큐에 notify가 전달되고 스레드2가 Writer Lock 점유
#### 리드-라이트 락의 장점
- 읽기 작업을 동시에 접근할 수 있어 읽기 작업이 많고 쓰기 작업이 적은 환경에서 CPU 및 메모리 자원을 효율적으로 활용 가능 
- Writer가 자원을 독점하여 작업을 수행하기 때문에 데이터 일관성이 보장됨 
#### 리드-라이트 락의 단점 
- Reader가 연속적으로 자원을 점유할 경우 Writer 기아(Starvation) 현상이 발생 
- Reader와 Writer의 상태를 구분해서 관리해야 하기 때문에 구현이 복잡함 
- 잘못된 순서로 락을 요청하면 데드락 발생 가능 

<br>


### 데드락 (DeadLock)
- 두 개 이상의 스레드/프로세스가 서로 자원을 점유한 채 상대방이 해제하기를  기다리며 무한히 대기하는 상태
#### 데드락 발생 조건 
- 상호 배제(Mutual Exclusion) : 자원은 한 번에 하나의 프로세스만 사용할 수 있음 
- 점유/대기 : 한 프로세스가 자원을 점유한 상태일 때 다른 자원은 대기 상태
- 비선점 방식 : 점유된 자원은 강제로 해제할 수 없음 
- 순환 대기 : 프로세스들이 서로 자원을 점유한 채 순환적으로 대기 
#### 중간 수준 스케줄링 
- Swap In/Swap Out 기법을 사용해서 데드락을 완화하는 기법 
- 프로세스를 일시적으로 디스크로 Swap Out 시켜 데드락을 회피하거나 해결 가능 
  - Swap Out : 특정 프로세스를 메모리에서 디스크로 이동시켜 일시적으로 실행을 중단
  - Swap In : swap out된 프로세스를 다시 메모리로 불러와 실행 재개
#### 중간 수준 스케줄링을 활용한 데드락 해결 방법
- 데드락이 감지되면 일부 프로세스를 Swap Out하여 자원을 해제시킴
- 데드락이 해결되면 다시 프로세스를 Swap In하여 실행 재개
- 우선순위가 낮은 프로세스를 Swap Out하여 중요한 작업이 먼저 수행되도록 함

<br>


### 교착상태 발생 흐름 이해하기 
- 프로세스 A가 자원 X를 점유함
- 프로세스 B가 자원 Y를 점유함
- 프로세스 A가 자원 Y를 요청하며 대기
- 프로세스 B가 자원 X를 요청하며 대기
- 두 프로세스가 서로 상대방이 점유한 자원을 기다리면서 무한 대기 상태 발생 


<br>


### 데드락 예방 시나리오 
- 자원 요청 순서 통일 : 모든 프로세스가 자원을 특정 순서대로 요청하도록 강제하기 
- 타임아웃 설정 : 프로세스가 일정 시간 동안 자원을 얻지 못하면 요청을 취소하기 대기 상태에서 벗어남
- 우선순위 기반 자원 할당 : 우선순위가 높은 프로세스에 먼저 자원을 할당함 
- 자원 요청 전에 모든 자원을 미리 할당 : 프로세스가 필요한 모든 자원을 한 번에 요청하고 사용이 끝나기 전까지 다른 요청을 하지 않음
- 중간 수준 스케줄링 사용 : 데드락이 감지 됐을 때 일부 프로세스를 디스크로 Swap Out하여 데드락 해소 

<br>


### 기아 상태 (Starvation)
- 특정 스레드/프로세스가 장시간 자원을 할당 받지 못하고 무한 대기하는 현상 
#### 기아 상태 해결 방법 
- 에이징 : 대기 시간이 길어질 수록 우선 순위를 증가 시키는 방식  
- 타임 슬라이스 보장 : 모든 프로세스에 일정한 CPU 실행 시간을 부여하는 방식 
- 최대 대기 시간 설정 : 일정 시간 이상 대기하면 강제로 실행되도록 대기 시간 설정 




<br>


### 세마포어 
- 공유 자원이 여러 개일 때 공유 자원 접근을 조절하는 동기화 기법 
- 여러 스레드/프로세스의 동시 접근을 허용하고 스레드간 소유권 제한이 없음 
- S : 현재 공유 자원에 접근할 수 있는 자원의 수 
- P(Wait) : 세마포어 값을 감소 시킴 -> 자원 점유 가능 여부 확인 
- V(Signal) : 세마포어 값을 증가 시킴 -> 자원 해제 
#### 세마포어 종류
- 이진 세마포어 : 0 또는 1 값만 가지며 단일 자원에 대한 접근을 제어함 
- 카운팅 세마포어 : 0 이상의 값만 가지며 다중 자원에 대한 접근을 제어함 
#### 세마포어 동작 방식
- 스레드1이 세마포어에 P연산 요청 
- 세마포어 값이 감소하고 스레드1의 요청 승인 : 스레드1이 힙1을 점유하여 작업 수행 
- 스레드2가 세마포어에 P연산 요청 
- 세마포어 값이 감소하고 스레드2의 요청 승인 : 스레드2가 힙2를 점유하여 작업 수행 
- 스레드2의 V연산 요청 및 점유 해제 : 세마포어 값이 증가하고 힙2가 해제되어 다른 스레드 접근이 가능해짐
#### 세마포어가 0일 경우 
- 스레드3이 세마포어에 P연산 요청 
- 세마포어 값이 -1로 감소 
- 스레드3의 요청이 대기큐로 들어감 

<br>





### 오늘의 회고
- 운영체제에서 자주 헷갈리는 개념인 락에 대해 정리해 보았다.
락은 멀티 스레드 환경에서 공유 자원의 동기화 문제를 해결하는 데 중요한 기법이지만 
사용 방법에 따라 여러 문제가 발생할 수 있다. 어떠한 동시화 방식이든 이점이 있다면 그만큼의 trade off가 존재하기 마련이다. 
따라서 단순히 특정 락을 사용하는 게 아니라 각 기법의 장단점에 대해 정확히 알고 상황에 맞는 해결책을 선택하는 것이 중요할 것 같다! 



<br>

참고 
- https://engineerinsight.tistory.com/288
- https://yoongrammer.tistory.com/63